# -*- coding: utf-8 -*-
"""Scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ewq8jOsqY0F72bcxHxTk9X5U6IYspcB4
"""

import pandas as pd
import requests
import string
from time import sleep
from bs4 import BeautifulSoup

def scrape_players_by_letter(letter):
    base_url = f"https://www.basketball-reference.com/players/{letter}/"
    page = requests.get(base_url)
    soup = BeautifulSoup(page.content, 'html.parser')

    # Extracting player information
    tag1 = soup.find_all('th', {'data-stat': 'player'})
    names = [tag.text.strip() for tag in tag1]

    tag2 = soup.find_all('td', {'data-stat': 'year_min'})
    start_year = [tag.text.strip() for tag in tag2]

    tag3 = soup.find_all('td', {'data-stat': 'year_max'})
    end_year = [tag.text.strip() for tag in tag3]

    tag4 = soup.find_all('td', {'data-stat': 'pos'})
    position = [tag.text.strip() for tag in tag4]

    tag5 = soup.find_all('td', {'data-stat': 'height'})
    height = [tag.text.strip() for tag in tag5]

    tag6 = soup.find_all('td', {'data-stat': 'weight'})
    weight = [tag.text.strip() for tag in tag6]

    tag7 = soup.find_all('td', {'data-stat': 'birth_date'})
    birth_day = [tag.text.strip() for tag in tag7]

    tag8 = soup.find_all('td', {'data-stat': 'colleges'})
    colleges = [tag.text.strip() for tag in tag8]

    tag9 = soup.select("#players > tbody > tr > td:nth-child(8) > a ")
    college_links = ["https://www.basketball-reference.com" + tag['href'] for tag in tag9]

    # Ensure all lists have the same length
    min_length = min(len(names), len(start_year), len(end_year), len(position), len(height), len(weight), len(birth_day), len(colleges), len(college_links))
    names = names[:min_length]
    start_year = start_year[:min_length]
    end_year = end_year[:min_length]
    position = position[:min_length]
    height = height[:min_length]
    weight = weight[:min_length]
    birth_day = birth_day[:min_length]
    colleges = colleges[:min_length]
    college_links = college_links[:min_length]

    df = pd.DataFrame({
        'Player': names,
        'Start Year': start_year,
        'End Year': end_year,
        'Position': position,
        'Height': height,
        'Weight': weight,
        'Birthday': birth_day,
        'Colleges': colleges,
        'College Link': college_links
    })

    return df

# get lowercase alphabets using string module
alph = list(string.ascii_lowercase)

# Iterate over all letters from 'a' to 'z'
all_dataframes = []

for letter in alph:
    sleep(1)
    df = scrape_players_by_letter(letter)
    all_dataframes.append(df)

result_df = pd.concat(all_dataframes, ignore_index=True)

result_df.head()

result_df.info()

result_df.to_csv('nba-players2.csv')

